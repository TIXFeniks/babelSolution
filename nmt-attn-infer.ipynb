{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64\n",
      "env: LIBRARY_PATH=/usr/local/cuda-8.0/lib64\n",
      "env: PATH=/usr/local/cuda-8.0/include:/usr/local/cuda-8.0/bin:/home/apanin/anaconda/bin:/opt/WinCC_OA/3.11/bin:/opt/pvss/pvss2_v3.8/bin:/home/apanin/anaconda/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/bin:/opt/fmc/bin:/group/online/bin:/group/online/scripts:/home/apanin/bin:/home/apanin/bin\n",
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: MKL_THREADING_LAYER=GNU\n",
      "env: THEANO_FLAGS=device=cpu,gpuarray.preallocate=0.99\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "%env LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64\n",
    "%env LIBRARY_PATH=/usr/local/cuda-8.0/lib64\n",
    "%env PATH=/usr/local/cuda-8.0/include:/usr/local/cuda-8.0/bin:/home/apanin/anaconda/bin:/opt/WinCC_OA/3.11/bin:/opt/pvss/pvss2_v3.8/bin:/home/apanin/anaconda/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/bin:/opt/fmc/bin:/group/online/bin:/group/online/scripts:/home/apanin/bin:/home/apanin/bin\n",
    "sys.path.insert(0,\"/home/apanin/rit_my/\")\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env MKL_THREADING_LAYER=GNU\n",
    "%env THEANO_FLAGS=device=cpu,gpuarray.preallocate=0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import lasagne.layers as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Version 1.1\n",
      "Language: en\n",
      "Number of threads: 1\n",
      "Tokenizer Version 1.1\n",
      "Language: de\n",
      "Number of threads: 1\n"
     ]
    }
   ],
   "source": [
    "!./preprocess.sh data/indomain_training/indomain.de-en.de preprocessed/ en de\n",
    "!./preprocess.sh data/indomain_training/indomain.de-en.en preprocessed/ de en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SRC_PATH = \"data/wmt/train.tok.bpe.32000.en\" \n",
    "DST_PATH = \"output.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_src = []\n",
    "with open(SRC_PATH, 'r') as f:\n",
    "    raw_src = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_src = raw_src[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_ends(sents):\n",
    "    for i in range(len(sents)):\n",
    "        sents[i] = sents[i][:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fix_ends(raw_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, sentences):\n",
    "        tokens = set()\n",
    "        for s in sentences:\n",
    "            tokens.update(s.split(' '))\n",
    "        self.tokens = [ \"__BOS__\",\"__EOS__\", \"__PAD__\"] + list(tokens)\n",
    "        self.EOS = 1\n",
    "        self.BOS = 0 # BOS should be zero to let the model generate starting with zero as an input\n",
    "        self.PAD = 2\n",
    "        self.len = len(self.tokens)\n",
    "        self.token2id = {token: i for i, token in enumerate(self.tokens)}\n",
    "    def tokenize(self, sentence):\n",
    "        if not sentence.endswith(\"__EOS__\"):\n",
    "            sentence += \" __EOS__\"\n",
    "        if not sentence.startswith(\"__BOS__\"):\n",
    "            sentence = \"__BOS__ \" + sentence\n",
    "        return [self.token2id[token] for token in sentence.split(' ')]\n",
    "    def detokenize(self, sentence):\n",
    "        return \" \".join([self.tokens[token] for token in sentence])\n",
    "    def tokenize_many(self, sentences):\n",
    "        return [self.tokenize(sent) for sent in sentences]\n",
    "    def detokenize_many(self, sentences):\n",
    "        return [self.detokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"vocabs.pkl\", 'rb') as f:\n",
    "    dst_voc = pkl.load(f)\n",
    "    src_voc = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = src_voc.tokenize_many(raw_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KERAS_BACKEND=theano\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%env KERAS_BACKEND=theano\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = T.matrix('token sequencea','int32')\n",
    "input_mask = T.neq(input_sequence, src_voc.PAD)\n",
    "\n",
    "target_values = T.matrix('actual next token','int32')\n",
    "target_mask = T.neq(target_values, dst_voc.PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CODE_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_in = lasagne.layers.InputLayer(shape=(None, None),input_var=input_sequence)\n",
    "l_mask = lasagne.layers.InputLayer(shape=(None, None),input_var=input_mask)\n",
    "\n",
    "#encoder\n",
    "l_emb = L.EmbeddingLayer(l_in, src_voc.len, 128)\n",
    "\n",
    "l_rnn = L.LSTMLayer(l_emb, 256, nonlinearity=T.tanh, mask_input= l_mask)\n",
    "l_rnn = L.concat([l_emb,l_rnn], axis=-1)\n",
    "l_encoded = l_rnn = L.LSTMLayer(l_rnn, CODE_SIZE, nonlinearity=T.tanh, mask_input= l_mask)\n",
    "\n",
    "l_trans = L.InputLayer((None, None), input_var= target_values[:,:-1])\n",
    "l_trans_mask = L.InputLayer((None, None), input_var= target_mask[:,:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.agent.recurrence import Recurrence\n",
    "from agentnet.memory import AttentionLayer,LSTMCell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.resolver import ProbabilisticResolver, GreedyResolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AutoLSTMCell:\n",
    "    def __init__(self, input_or_inputs, num_units = None, *args, name=None, **kwargs):\n",
    "        self.p_cell = L.InputLayer((None, num_units), \n",
    "                       name=\"previous cell state\" if name == None else name + \" previous cell state\")\n",
    "        self.p_out = L.InputLayer((None, num_units), \n",
    "                       name=\"previous out state\" if name == None else name + \" previous out state\")\n",
    "        self.cell, self.out = LSTMCell(self.p_cell, self.p_out, input_or_inputs, num_units, *args,name=name, **kwargs)\n",
    "    def get_automatic_updates(self):\n",
    "        return {self.cell: self.p_cell, self.out: self.p_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TemperatureResolver(ProbabilisticResolver):\n",
    "    def __init__(self, incoming, tau, **kwargs):\n",
    "        self.tau = tau\n",
    "        super(TemperatureResolver, self).__init__(incoming,**kwargs)\n",
    "    def get_output_for(self, policy, **kwargs):\n",
    "        policy = policy ** (1/self.tau)\n",
    "        policy /= policy.sum()\n",
    "        return super(TemperatureResolver, self).get_output_for(policy, **kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class decoder_step:\n",
    "    #inputs\n",
    "    encoder = L.InputLayer((None, None ,CODE_SIZE), name='encoded sequence')\n",
    "    encoder_mask = L.InputLayer((None, None), name='encoded sequence')\n",
    "    \n",
    "    inp = L.InputLayer((None,),name='current character')\n",
    "    \n",
    "    l_target_emb = L.EmbeddingLayer(inp, dst_voc.len, 128)\n",
    "    \n",
    "    #recurrent part\n",
    "    \n",
    "    l_rnn1 = AutoLSTMCell(l_target_emb, 128, name=\"lstm1\")\n",
    "    \n",
    "    query = L.DenseLayer(l_rnn1.out, 128, nonlinearity=None)\n",
    "    attn = AttentionLayer(encoder, query, 128, mask_input= encoder_mask)['attn']\n",
    "    \n",
    "    l_rnn = L.concat([attn, l_rnn1.out, l_target_emb])\n",
    "    \n",
    "    l_rnn2 = AutoLSTMCell(l_rnn, 128, name=\"lstm1\")\n",
    "    \n",
    "    next_token_probas = L.DenseLayer(l_rnn2.out, dst_voc.len, nonlinearity=T.nnet.softmax)\n",
    "    \n",
    "    #pick next token from predicted probas\n",
    "    next_token = ProbabilisticResolver(next_token_probas)\n",
    "    \n",
    "    tau = T.scalar(\"sample temperature\", \"float32\")\n",
    "    \n",
    "    next_token_temperatured = TemperatureResolver(next_token_probas, tau)\n",
    "    next_token_greedy = GreedyResolver(next_token_probas)\n",
    "    \n",
    "    auto_updates = {**l_rnn1.get_automatic_updates(),\n",
    "                    **l_rnn2.get_automatic_updates()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_steps = T.scalar(dtype='int32')\n",
    "feedback_loop = Recurrence(\n",
    "    state_variables=OrderedDict({**decoder_step.auto_updates,\n",
    "                     decoder_step.next_token:decoder_step.inp}),\n",
    "    tracked_outputs=[decoder_step.next_token_probas, decoder_step.next_token],\n",
    "    input_nonsequences= OrderedDict({decoder_step.encoder: l_encoded, decoder_step.encoder_mask: l_mask} ),\n",
    "    batch_size=input_sequence.shape[0],\n",
    "    n_steps=n_steps,\n",
    "    unroll_scan=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, W_in_to_ingate, W_hid_to_ingate, b_ingate, W_in_to_forgetgate, W_hid_to_forgetgate, b_forgetgate, W_in_to_cell, W_hid_to_cell, b_cell, W_in_to_outgate, W_hid_to_outgate, b_outgate, W_cell_to_ingate, W_cell_to_forgetgate, W_cell_to_outgate, W_in_to_ingate, W_hid_to_ingate, b_ingate, W_in_to_forgetgate, W_hid_to_forgetgate, b_forgetgate, W_in_to_cell, W_hid_to_cell, b_cell, W_in_to_outgate, W_hid_to_outgate, b_outgate, W_cell_to_ingate, W_cell_to_forgetgate, W_cell_to_outgate, W, lstm1.b_to_ingate, lstm1.W_lstm1 previous out state_to_ingate, lstm1.W_ctrl1_to_ingate, lstm1.b_to_forgetgate, lstm1.W_lstm1 previous out state_to_forgetgate, lstm1.W_ctrl1_to_forgetgate, lstm1.b_to_cell, lstm1.W_lstm1 previous out state_to_cell, lstm1.W_ctrl1_to_cell, lstm1.b_to_outgate, lstm1.W_lstm1 previous out state_to_outgate, lstm1.W_ctrl1_to_outgate, lstm1.W_cell_to_ingate_peephole.scales, lstm1.W_cell_to_forgetgate_peephole.scales, lstm1.W_cell_to_outgate_peephole.scales, W, b, enc_to_hid, dec_to_hid, hid_to_logit, lstm1.b_to_ingate, lstm1.W_lstm1 previous out state_to_ingate, lstm1.W_ctrl1_to_ingate, lstm1.b_to_forgetgate, lstm1.W_lstm1 previous out state_to_forgetgate, lstm1.W_ctrl1_to_forgetgate, lstm1.b_to_cell, lstm1.W_lstm1 previous out state_to_cell, lstm1.W_ctrl1_to_cell, lstm1.b_to_outgate, lstm1.W_lstm1 previous out state_to_outgate, lstm1.W_ctrl1_to_outgate, lstm1.W_cell_to_ingate_peephole.scales, lstm1.W_cell_to_forgetgate_peephole.scales, lstm1.W_cell_to_outgate_peephole.scales, W, b]\n"
     ]
    }
   ],
   "source": [
    "# Model weights\n",
    "weights = lasagne.layers.get_all_params(feedback_loop,trainable=True)\n",
    "print (weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generated_tokens = L.get_output(feedback_loop[decoder_step.next_token])\n",
    "\n",
    "generate_sample = theano.function([input_sequence ,n_steps],generated_tokens,\n",
    "                                  updates=feedback_loop.get_automatic_updates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feedback_loop_greedy = Recurrence(\n",
    "    state_variables=OrderedDict({**decoder_step.auto_updates,\n",
    "                     decoder_step.next_token_greedy:decoder_step.inp}),\n",
    "    tracked_outputs=[decoder_step.next_token_probas, decoder_step.next_token_greedy],\n",
    "    input_nonsequences= OrderedDict({decoder_step.encoder: l_encoded, decoder_step.encoder_mask: l_mask} ),\n",
    "    batch_size=input_sequence.shape[0],\n",
    "    n_steps=n_steps,\n",
    "    unroll_scan=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generated_tokens_greedy = L.get_output(feedback_loop_greedy[decoder_step.next_token_greedy])\n",
    "\n",
    "generate_sample_greedy = theano.function([input_sequence ,n_steps],generated_tokens_greedy,\n",
    "                                  updates=feedback_loop_greedy.get_automatic_updates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feedback_loop_temp = Recurrence(\n",
    "    state_variables=OrderedDict({**decoder_step.auto_updates,\n",
    "                     decoder_step.next_token_temperatured:decoder_step.inp}),\n",
    "    tracked_outputs=[decoder_step.next_token_probas, decoder_step.next_token_temperatured],\n",
    "    input_nonsequences= OrderedDict({decoder_step.encoder: l_encoded, decoder_step.encoder_mask: l_mask} ),\n",
    "    batch_size=input_sequence.shape[0],\n",
    "    n_steps=n_steps,\n",
    "    unroll_scan=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generated_tokens_temp = L.get_output(feedback_loop_temp[decoder_step.next_token_temperatured])\n",
    "\n",
    "generate_sample_temp = theano.function([input_sequence ,n_steps, decoder_step.tau],generated_tokens_temp,\n",
    "                                  updates=feedback_loop_temp.get_automatic_updates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_dict = {str(i): weight for i, weight in enumerate(weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with np.load(\"weights.npz\") as f:\n",
    "    for key in weights_dict:\n",
    "        weights_dict[key].set_value(f[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from batch_iterator import iterate_minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate(srcs, N=MAX_LEN,t=1, greedy=False):\n",
    "    srcs = [src_voc.tokenize(src) for src in srcs]\n",
    "    if len(srcs) > 1:\n",
    "        srcs = pad_sequences(srcs, value= src_voc.PAD, maxlen=MAX_LEN, padding=\"post\")\n",
    "    sample_ix = generate_sample_greedy(srcs, N) if greedy else generate_sample_temp(srcs, N, t)\n",
    "    random_snippet = dst_voc.detokenize_many(sample_ix)\n",
    "    res = []\n",
    "    for sent in random_snippet:\n",
    "        if sent.find(\"__EOS__\") > 0:\n",
    "            res.append(sent[:sent.find(\"__EOS__\")].replace(\"@@ \", \"\"))\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/1000 [00:00<11:18,  1.47it/s]\u001b[A\n",
      "  0%|          | 2/1000 [00:01<11:12,  1.48it/s]\u001b[A\n",
      "  0%|          | 3/1000 [00:02<11:08,  1.49it/s]\u001b[A\n",
      "  0%|          | 4/1000 [00:02<11:00,  1.51it/s]\u001b[A\n",
      "  0%|          | 5/1000 [00:03<11:01,  1.50it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(DST_PATH, 'w') as f:\n",
    "    for start in tqdm(range(0, len(raw_src), 1)):\n",
    "        f.writelines(translate(raw_src[start: start+1], t=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translating_ind = np.random.randint(len(raw_src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is not just the fault of the Commission , but I believe that we need to take action more quickly so as to achieve harmonisation in this area as well \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Das ist nicht nur die Kommission , aber ich glaube , dass wir uns auf die Notwendigkeit , die wir in diesem Bereich zu erreichen , um die Lösung zu gewährleisten , um die Lösung zu gewährleisten  ']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw_src[translating_ind])\n",
    "translate([raw_src[translating_ind]], t=0.1, greedy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
