{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64\n",
      "env: LIBRARY_PATH=/usr/local/cuda-8.0/lib64\n",
      "env: PATH=/usr/local/cuda-8.0/include:/usr/local/cuda-8.0/bin:/home/apanin/anaconda/bin:/opt/WinCC_OA/3.11/bin:/opt/pvss/pvss2_v3.8/bin:/home/apanin/anaconda/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/bin:/opt/fmc/bin:/group/online/bin:/group/online/scripts:/home/apanin/bin:/home/apanin/bin\n",
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: MKL_THREADING_LAYER=GNU\n",
      "env: THEANO_FLAGS=device=cpu,gpuarray.preallocate=0.99\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "%env LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64\n",
    "%env LIBRARY_PATH=/usr/local/cuda-8.0/lib64\n",
    "%env PATH=/usr/local/cuda-8.0/include:/usr/local/cuda-8.0/bin:/home/apanin/anaconda/bin:/opt/WinCC_OA/3.11/bin:/opt/pvss/pvss2_v3.8/bin:/home/apanin/anaconda/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/bin:/opt/fmc/bin:/group/online/bin:/group/online/scripts:/home/apanin/bin:/home/apanin/bin\n",
    "sys.path.insert(0,\"/home/apanin/rit_my/\")\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env MKL_THREADING_LAYER=GNU\n",
    "%env THEANO_FLAGS=device=cpu,gpuarray.preallocate=0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import lasagne.layers as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Version 1.1\n",
      "Language: en\n",
      "Number of threads: 1\n",
      "Tokenizer Version 1.1\n",
      "Language: de\n",
      "Number of threads: 1\n"
     ]
    }
   ],
   "source": [
    "!./preprocess.sh data/indomain_training/indomain.de-en.de preprocessed/ en de\n",
    "!./preprocess.sh data/indomain_training/indomain.de-en.en preprocessed/ de en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SRC_PATH = \"data/wmt/train.tok.bpe.32000.en\" \n",
    "DST_PATH = \"output.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_src = []\n",
    "with open(SRC_PATH, 'r') as f:\n",
    "    raw_src = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_src = raw_src[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_ends(sents):\n",
    "    for i in range(len(sents)):\n",
    "        sents[i] = sents[i][:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fix_ends(raw_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, sentences):\n",
    "        tokens = set()\n",
    "        for s in sentences:\n",
    "            tokens.update(s.split(' '))\n",
    "        self.tokens = [ \"__BOS__\",\"__EOS__\", \"__PAD__\"] + list(tokens)\n",
    "        self.EOS = 1\n",
    "        self.BOS = 0 # BOS should be zero to let the model generate starting with zero as an input\n",
    "        self.PAD = 2\n",
    "        self.len = len(self.tokens)\n",
    "        self.token2id = {token: i for i, token in enumerate(self.tokens)}\n",
    "    def tokenize(self, sentence):\n",
    "        if not sentence.endswith(\"__EOS__\"):\n",
    "            sentence += \" __EOS__\"\n",
    "        if not sentence.startswith(\"__BOS__\"):\n",
    "            sentence = \"__BOS__ \" + sentence\n",
    "        return [self.token2id[token] for token in sentence.split(' ')]\n",
    "    def detokenize(self, sentence):\n",
    "        return \" \".join([self.tokens[token] for token in sentence])\n",
    "    def tokenize_many(self, sentences):\n",
    "        return [self.tokenize(sent) for sent in sentences]\n",
    "    def detokenize_many(self, sentences):\n",
    "        return [self.detokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"vocabs.pkl\", 'rb') as f:\n",
    "    dst_voc = pkl.load(f)\n",
    "    src_voc = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = src_voc.tokenize_many(raw_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KERAS_BACKEND=theano\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%env KERAS_BACKEND=theano\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = T.matrix('token sequencea','int32')\n",
    "input_mask = T.neq(input_sequence, src_voc.PAD)\n",
    "\n",
    "target_values = T.matrix('actual next token','int32')\n",
    "target_mask = T.neq(target_values, dst_voc.PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CODE_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_in = lasagne.layers.InputLayer(shape=(None, None),input_var=input_sequence)\n",
    "l_mask = lasagne.layers.InputLayer(shape=(None, None),input_var=input_mask)\n",
    "\n",
    "#encoder\n",
    "l_emb = L.EmbeddingLayer(l_in, src_voc.len, 128)\n",
    "\n",
    "l_rnn = L.LSTMLayer(l_emb, 256, nonlinearity=T.tanh, mask_input= l_mask)\n",
    "l_rnn = L.concat([l_emb,l_rnn], axis=-1)\n",
    "l_encoded = l_rnn = L.LSTMLayer(l_rnn, CODE_SIZE, nonlinearity=T.tanh, mask_input= l_mask)\n",
    "\n",
    "l_trans = L.InputLayer((None, None), input_var= target_values[:,:-1])\n",
    "l_trans_mask = L.InputLayer((None, None), input_var= target_mask[:,:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.agent.recurrence import Recurrence\n",
    "from agentnet.memory import AttentionLayer,LSTMCell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.resolver import ProbabilisticResolver, GreedyResolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AutoLSTMCell:\n",
    "    def __init__(self, input_or_inputs, num_units = None, *args, name=None, **kwargs):\n",
    "        self.p_cell = L.InputLayer((None, num_units), \n",
    "                       name=\"previous cell state\" if name == None else name + \" previous cell state\")\n",
    "        self.p_out = L.InputLayer((None, num_units), \n",
    "                       name=\"previous out state\" if name == None else name + \" previous out state\")\n",
    "        self.cell, self.out = LSTMCell(self.p_cell, self.p_out, input_or_inputs, num_units, *args,name=name, **kwargs)\n",
    "    def get_automatic_updates(self):\n",
    "        return {self.cell: self.p_cell, self.out: self.p_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TemperatureResolver(ProbabilisticResolver):\n",
    "    def __init__(self, incoming, tau, **kwargs):\n",
    "        self.tau = tau\n",
    "        super(TemperatureResolver, self).__init__(incoming,**kwargs)\n",
    "    def get_output_for(self, policy, **kwargs):\n",
    "        policy = policy ** (1/self.tau)\n",
    "        policy /= policy.sum()\n",
    "        return super(TemperatureResolver, self).get_output_for(policy, **kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class decoder_step:\n",
    "    #inputs\n",
    "    encoder = L.InputLayer((None, None ,CODE_SIZE), name='encoded sequence')\n",
    "    encoder_mask = L.InputLayer((None, None), name='encoded sequence')\n",
    "    \n",
    "    inp = L.InputLayer((None,),name='current character')\n",
    "    \n",
    "    l_target_emb = L.EmbeddingLayer(inp, dst_voc.len, 128)\n",
    "    \n",
    "    #recurrent part\n",
    "    \n",
    "    l_rnn1 = AutoLSTMCell(l_target_emb, 128, name=\"lstm1\")\n",
    "    \n",
    "    query = L.DenseLayer(l_rnn1.out, 128, nonlinearity=None)\n",
    "    attn = AttentionLayer(encoder, query, 128, mask_input= encoder_mask)['attn']\n",
    "    \n",
    "    l_rnn = L.concat([attn, l_rnn1.out, l_target_emb])\n",
    "    \n",
    "    l_rnn2 = AutoLSTMCell(l_rnn, 128, name=\"lstm1\")\n",
    "    \n",
    "    next_token_probas = L.DenseLayer(l_rnn2.out, dst_voc.len, nonlinearity=T.nnet.softmax)\n",
    "    \n",
    "    #pick next token from predicted probas\n",
    "    next_token = ProbabilisticResolver(next_token_probas)\n",
    "    \n",
    "    tau = T.scalar(\"sample temperature\", \"float32\")\n",
    "    \n",
    "    next_token_temperatured = TemperatureResolver(next_token_probas, tau)\n",
    "    next_token_greedy = GreedyResolver(next_token_probas)\n",
    "    \n",
    "    auto_updates = {**l_rnn1.get_automatic_updates(),\n",
    "                    **l_rnn2.get_automatic_updates()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_steps = T.scalar(dtype='int32')\n",
    "feedback_loop = Recurrence(\n",
    "    state_variables=OrderedDict({**decoder_step.auto_updates,\n",
    "                     decoder_step.next_token:decoder_step.inp}),\n",
    "    tracked_outputs=[decoder_step.next_token_probas, decoder_step.next_token],\n",
    "    input_nonsequences= OrderedDict({decoder_step.encoder: l_encoded, decoder_step.encoder_mask: l_mask} ),\n",
    "    batch_size=input_sequence.shape[0],\n",
    "    n_steps=n_steps,\n",
    "    unroll_scan=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, W_in_to_ingate, W_hid_to_ingate, b_ingate, W_in_to_forgetgate, W_hid_to_forgetgate, b_forgetgate, W_in_to_cell, W_hid_to_cell, b_cell, W_in_to_outgate, W_hid_to_outgate, b_outgate, W_cell_to_ingate, W_cell_to_forgetgate, W_cell_to_outgate, W_in_to_ingate, W_hid_to_ingate, b_ingate, W_in_to_forgetgate, W_hid_to_forgetgate, b_forgetgate, W_in_to_cell, W_hid_to_cell, b_cell, W_in_to_outgate, W_hid_to_outgate, b_outgate, W_cell_to_ingate, W_cell_to_forgetgate, W_cell_to_outgate, W, lstm1.b_to_ingate, lstm1.W_lstm1 previous out state_to_ingate, lstm1.W_ctrl1_to_ingate, lstm1.b_to_forgetgate, lstm1.W_lstm1 previous out state_to_forgetgate, lstm1.W_ctrl1_to_forgetgate, lstm1.b_to_cell, lstm1.W_lstm1 previous out state_to_cell, lstm1.W_ctrl1_to_cell, lstm1.b_to_outgate, lstm1.W_lstm1 previous out state_to_outgate, lstm1.W_ctrl1_to_outgate, lstm1.W_cell_to_ingate_peephole.scales, lstm1.W_cell_to_forgetgate_peephole.scales, lstm1.W_cell_to_outgate_peephole.scales, W, b, enc_to_hid, dec_to_hid, hid_to_logit, lstm1.b_to_ingate, lstm1.W_lstm1 previous out state_to_ingate, lstm1.W_ctrl1_to_ingate, lstm1.b_to_forgetgate, lstm1.W_lstm1 previous out state_to_forgetgate, lstm1.W_ctrl1_to_forgetgate, lstm1.b_to_cell, lstm1.W_lstm1 previous out state_to_cell, lstm1.W_ctrl1_to_cell, lstm1.b_to_outgate, lstm1.W_lstm1 previous out state_to_outgate, lstm1.W_ctrl1_to_outgate, lstm1.W_cell_to_ingate_peephole.scales, lstm1.W_cell_to_forgetgate_peephole.scales, lstm1.W_cell_to_outgate_peephole.scales, W, b]\n"
     ]
    }
   ],
   "source": [
    "# Model weights\n",
    "weights = lasagne.layers.get_all_params(feedback_loop,trainable=True)\n",
    "print (weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generated_tokens = L.get_output(feedback_loop[decoder_step.next_token])\n",
    "\n",
    "generate_sample = theano.function([input_sequence ,n_steps],generated_tokens,\n",
    "                                  updates=feedback_loop.get_automatic_updates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feedback_loop_greedy = Recurrence(\n",
    "    state_variables=OrderedDict({**decoder_step.auto_updates,\n",
    "                     decoder_step.next_token_greedy:decoder_step.inp}),\n",
    "    tracked_outputs=[decoder_step.next_token_probas, decoder_step.next_token_greedy],\n",
    "    input_nonsequences= OrderedDict({decoder_step.encoder: l_encoded, decoder_step.encoder_mask: l_mask} ),\n",
    "    batch_size=input_sequence.shape[0],\n",
    "    n_steps=n_steps,\n",
    "    unroll_scan=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generated_tokens_greedy = L.get_output(feedback_loop_greedy[decoder_step.next_token_greedy])\n",
    "\n",
    "generate_sample_greedy = theano.function([input_sequence ,n_steps],generated_tokens_greedy,\n",
    "                                  updates=feedback_loop_greedy.get_automatic_updates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feedback_loop_temp = Recurrence(\n",
    "    state_variables=OrderedDict({**decoder_step.auto_updates,\n",
    "                     decoder_step.next_token_temperatured:decoder_step.inp}),\n",
    "    tracked_outputs=[decoder_step.next_token_probas, decoder_step.next_token_temperatured],\n",
    "    input_nonsequences= OrderedDict({decoder_step.encoder: l_encoded, decoder_step.encoder_mask: l_mask} ),\n",
    "    batch_size=input_sequence.shape[0],\n",
    "    n_steps=n_steps,\n",
    "    unroll_scan=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generated_tokens_temp = L.get_output(feedback_loop_temp[decoder_step.next_token_temperatured])\n",
    "\n",
    "generate_sample_temp = theano.function([input_sequence ,n_steps, decoder_step.tau],generated_tokens_temp,\n",
    "                                  updates=feedback_loop_temp.get_automatic_updates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_dict = {str(i): weight for i, weight in enumerate(weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with np.load(\"weights.npz\") as f:\n",
    "    for key in weights_dict:\n",
    "        weights_dict[key].set_value(f[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from batch_iterator import iterate_minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate(srcs, N=MAX_LEN,t=1, greedy=False):\n",
    "    srcs = [src_voc.tokenize(src) for src in srcs]\n",
    "    if len(srcs) > 1:\n",
    "        srcs = pad_sequences(srcs, value= src_voc.PAD, maxlen=MAX_LEN, padding=\"post\")\n",
    "    sample_ix = generate_sample_greedy(srcs, N) if greedy else generate_sample_temp(srcs, N, t)\n",
    "    random_snippet = dst_voc.detokenize_many(sample_ix)\n",
    "    res = []\n",
    "    for sent in random_snippet:\n",
    "        if sent.find(\"__EOS__\") > 0:\n",
    "            res.append(sent[:sent.find(\"__EOS__\")].replace(\"@@ \", \"\"))\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/1000 [00:00<12:51,  1.29it/s]\u001b[A\n",
      "  0%|          | 2/1000 [00:01<12:53,  1.29it/s]\u001b[A\n",
      "  0%|          | 3/1000 [00:02<12:59,  1.28it/s]\u001b[A\n",
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/apanin/anaconda/envs/py35/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/apanin/anaconda/envs/py35/lib/python3.5/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/apanin/anaconda/envs/py35/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      " 54%|█████▎    | 536/1000 [06:13<05:23,  1.43it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-dc256379b806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDST_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_src\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwritelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_src\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-61-b291b92d550d>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(srcs, N, t, greedy)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrcs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msrcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msrc_voc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPAD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msample_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_sample_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgreedy\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mgenerate_sample_temp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mrandom_snippet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdst_voc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetokenize_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py35/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py35/lib/python3.5/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    961\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    962\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py35/lib/python3.5/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mp\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    950\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m                                                 self, node)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/home/apanin/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.4.1708-Core-x86_64-3.5.4-64/scan_perform/mod.cpp:6946)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py35/lib/python3.5/site-packages/theano/tensor/type.py\u001b[0m in \u001b[0;36mvalue_zeros\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mvalue_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m         \"\"\"\n\u001b[1;32m    554\u001b[0m         \u001b[0mCreate\u001b[0m \u001b[0man\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0mndarray\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mof\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "with open(DST_PATH, 'w') as f:\n",
    "    for start in tqdm(range(0, len(raw_src), 1)):\n",
    "        f.writelines(translate(raw_src[start: start+1], t=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translating_ind = np.random.randint(len(raw_src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is not just the fault of the Commission , but I believe that we need to take action more quickly so as to achieve harmonisation in this area as well \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Das ist nicht nur die Kommission , aber ich glaube , dass wir uns auf die Notwendigkeit , die wir in diesem Bereich zu erreichen , um die Lösung zu gewährleisten , um die Lösung zu gewährleisten  ']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw_src[translating_ind])\n",
    "translate([raw_src[translating_ind]], t=0.1, greedy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
