{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import ewma\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from vocab import Vocab\n",
    "from src.training_utils import *\n",
    "from lib.tensor_utils import infer_mask, initialize_uninitialized_variables, all_shapes_equal\n",
    "\n",
    "from models.transformer_fused import Model\n",
    "from models.transformer_lm import TransformerLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "model_name = 'transformer'\n",
    "\n",
    "config = {\n",
    "    'data_path': '../data_small',\n",
    "    'src_lm_path': '../trained_models/lm1/model.npz',\n",
    "    'target_lm_path': '../trained_models/lm2/model.npz',\n",
    "    'hp_file_path': '../hp_files/trans_default.json',\n",
    "    'use_early_stopping': True,\n",
    "    'early_stopping_last_n': 10,\n",
    "    'max_epochs': 1000,\n",
    "    'max_time_seconds': 1200,\n",
    "    'batch_size_for_inference': 16,\n",
    "    'max_len': 200,\n",
    "    'validate_every_num_epochs': 5,\n",
    "    #'warm_up_num_epochs': 10,\n",
    "    'gpu_memory_fraction': 0.5,\n",
    "    'min_interval_between_saves': 3,\n",
    "    'max_num_models': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = '../trained_models/{}'.format(model_name)\n",
    "if not os.path.isdir('trained_models'): os.mkdir('trained_models')\n",
    "if not os.path.isdir(model_path): os.mkdir(model_path)\n",
    "\n",
    "src_train_path = '{}/bpe_parallel_train1.txt'.format(config.get('data_path'))\n",
    "dst_train_path = '{}/bpe_parallel_train2.txt'.format(config.get('data_path'))\n",
    "src_val_path = '{}/bpe_parallel_val1.txt'.format(config.get('data_path'))\n",
    "dst_val_path = '{}/bpe_parallel_val2.txt'.format(config.get('data_path'))\n",
    "\n",
    "src_train = open(src_train_path, 'r', encoding='utf-8').read().splitlines()\n",
    "dst_train = open(dst_train_path, 'r', encoding='utf-8').read().splitlines()\n",
    "src_val = open(src_val_path, 'r', encoding='utf-8').read().splitlines()\n",
    "dst_val = open(dst_val_path, 'r', encoding='utf-8').read().splitlines()\n",
    "\n",
    "inp_voc = Vocab.from_file('{}/1.voc'.format(config.get('data_path')))\n",
    "out_voc = Vocab.from_file('{}/2.voc'.format(config.get('data_path')))\n",
    "max_len = config.get('max_len', 200)\n",
    "\n",
    "max_num_models = config.get('max_num_models', 4)\n",
    "\n",
    "hp = json.load(open(config.get('hp_file_path'), 'r', encoding='utf-8')) if config.get('hp_file_path') else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm2/emb_inp_bias:0\n",
      "lm2/main/emb_inp/mat:0\n",
      "lm2/main/enc_attn-0/mem_conv/W:0\n",
      "lm2/main/enc_attn-0/mem_conv/b:0\n",
      "lm2/main/enc_attn-0/out_conv/W:0\n",
      "lm2/main/enc_attn-0/out_conv/b:0\n",
      "lm2/main/enc_attn-0/layer_norm/scale:0\n",
      "lm2/main/enc_attn-0/layer_norm/bias:0\n",
      "lm2/main/enc_attn-1/mem_conv/W:0\n",
      "lm2/main/enc_attn-1/mem_conv/b:0\n",
      "lm2/main/enc_attn-1/out_conv/W:0\n",
      "lm2/main/enc_attn-1/out_conv/b:0\n",
      "lm2/main/enc_attn-1/layer_norm/scale:0\n",
      "lm2/main/enc_attn-1/layer_norm/bias:0\n",
      "lm2/main/enc_attn-2/mem_conv/W:0\n",
      "lm2/main/enc_attn-2/mem_conv/b:0\n",
      "lm2/main/enc_attn-2/out_conv/W:0\n",
      "lm2/main/enc_attn-2/out_conv/b:0\n",
      "lm2/main/enc_attn-2/layer_norm/scale:0\n",
      "lm2/main/enc_attn-2/layer_norm/bias:0\n",
      "lm2/main/enc_attn-3/mem_conv/W:0\n",
      "lm2/main/enc_attn-3/mem_conv/b:0\n",
      "lm2/main/enc_attn-3/out_conv/W:0\n",
      "lm2/main/enc_attn-3/out_conv/b:0\n",
      "lm2/main/enc_attn-3/layer_norm/scale:0\n",
      "lm2/main/enc_attn-3/layer_norm/bias:0\n",
      "lm2/main/enc_ffn-0/conv1/W:0\n",
      "lm2/main/enc_ffn-0/conv1/b:0\n",
      "lm2/main/enc_ffn-0/conv2/W:0\n",
      "lm2/main/enc_ffn-0/conv2/b:0\n",
      "lm2/main/enc_ffn-0/layer_norm/scale:0\n",
      "lm2/main/enc_ffn-0/layer_norm/bias:0\n",
      "lm2/main/enc_ffn-1/conv1/W:0\n",
      "lm2/main/enc_ffn-1/conv1/b:0\n",
      "lm2/main/enc_ffn-1/conv2/W:0\n",
      "lm2/main/enc_ffn-1/conv2/b:0\n",
      "lm2/main/enc_ffn-1/layer_norm/scale:0\n",
      "lm2/main/enc_ffn-1/layer_norm/bias:0\n",
      "lm2/main/enc_ffn-2/conv1/W:0\n",
      "lm2/main/enc_ffn-2/conv1/b:0\n",
      "lm2/main/enc_ffn-2/conv2/W:0\n",
      "lm2/main/enc_ffn-2/conv2/b:0\n",
      "lm2/main/enc_ffn-2/layer_norm/scale:0\n",
      "lm2/main/enc_ffn-2/layer_norm/bias:0\n",
      "lm2/main/enc_ffn-3/conv1/W:0\n",
      "lm2/main/enc_ffn-3/conv1/b:0\n",
      "lm2/main/enc_ffn-3/conv2/W:0\n",
      "lm2/main/enc_ffn-3/conv2/b:0\n",
      "lm2/main/enc_ffn-3/layer_norm/scale:0\n",
      "lm2/main/enc_ffn-3/layer_norm/bias:0\n",
      "lm2/main/enc_out_norm/scale:0\n",
      "lm2/main/enc_out_norm/bias:0\n",
      "lm2/logits/W:0\n",
      "lm2/logits/b:0\n",
      "lm1/emb_inp_bias:0\n",
      "lm1/main/emb_inp/mat:0\n",
      "lm1/main/enc_attn-0/mem_conv/W:0\n",
      "lm1/main/enc_attn-0/mem_conv/b:0\n",
      "lm1/main/enc_attn-0/out_conv/W:0\n",
      "lm1/main/enc_attn-0/out_conv/b:0\n",
      "lm1/main/enc_attn-0/layer_norm/scale:0\n",
      "lm1/main/enc_attn-0/layer_norm/bias:0\n",
      "lm1/main/enc_attn-1/mem_conv/W:0\n",
      "lm1/main/enc_attn-1/mem_conv/b:0\n",
      "lm1/main/enc_attn-1/out_conv/W:0\n",
      "lm1/main/enc_attn-1/out_conv/b:0\n",
      "lm1/main/enc_attn-1/layer_norm/scale:0\n",
      "lm1/main/enc_attn-1/layer_norm/bias:0\n",
      "lm1/main/enc_attn-2/mem_conv/W:0\n",
      "lm1/main/enc_attn-2/mem_conv/b:0\n",
      "lm1/main/enc_attn-2/out_conv/W:0\n",
      "lm1/main/enc_attn-2/out_conv/b:0\n",
      "lm1/main/enc_attn-2/layer_norm/scale:0\n",
      "lm1/main/enc_attn-2/layer_norm/bias:0\n",
      "lm1/main/enc_attn-3/mem_conv/W:0\n",
      "lm1/main/enc_attn-3/mem_conv/b:0\n",
      "lm1/main/enc_attn-3/out_conv/W:0\n",
      "lm1/main/enc_attn-3/out_conv/b:0\n",
      "lm1/main/enc_attn-3/layer_norm/scale:0\n",
      "lm1/main/enc_attn-3/layer_norm/bias:0\n",
      "lm1/main/enc_ffn-0/conv1/W:0\n",
      "lm1/main/enc_ffn-0/conv1/b:0\n",
      "lm1/main/enc_ffn-0/conv2/W:0\n",
      "lm1/main/enc_ffn-0/conv2/b:0\n",
      "lm1/main/enc_ffn-0/layer_norm/scale:0\n",
      "lm1/main/enc_ffn-0/layer_norm/bias:0\n",
      "lm1/main/enc_ffn-1/conv1/W:0\n",
      "lm1/main/enc_ffn-1/conv1/b:0\n",
      "lm1/main/enc_ffn-1/conv2/W:0\n",
      "lm1/main/enc_ffn-1/conv2/b:0\n",
      "lm1/main/enc_ffn-1/layer_norm/scale:0\n",
      "lm1/main/enc_ffn-1/layer_norm/bias:0\n",
      "lm1/main/enc_ffn-2/conv1/W:0\n",
      "lm1/main/enc_ffn-2/conv1/b:0\n",
      "lm1/main/enc_ffn-2/conv2/W:0\n",
      "lm1/main/enc_ffn-2/conv2/b:0\n",
      "lm1/main/enc_ffn-2/layer_norm/scale:0\n",
      "lm1/main/enc_ffn-2/layer_norm/bias:0\n",
      "lm1/main/enc_ffn-3/conv1/W:0\n",
      "lm1/main/enc_ffn-3/conv1/b:0\n",
      "lm1/main/enc_ffn-3/conv2/W:0\n",
      "lm1/main/enc_ffn-3/conv2/b:0\n",
      "lm1/main/enc_ffn-3/layer_norm/scale:0\n",
      "lm1/main/enc_ffn-3/layer_norm/bias:0\n",
      "lm1/main/enc_out_norm/scale:0\n",
      "lm1/main/enc_out_norm/bias:0\n",
      "lm1/logits/W:0\n",
      "lm1/logits/b:0\n"
     ]
    }
   ],
   "source": [
    "gpu_options = create_gpu_options(config)\n",
    "\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "lm = TransformerLM('lm2', out_voc, **hp)\n",
    "if config.get('target_lm_path'):\n",
    "    lm_weights = np.load(config.get('target_lm_path'))\n",
    "    ops = []\n",
    "    for w in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, lm.name):\n",
    "        if w.name in lm_weights:\n",
    "            ops.append(tf.assign(w, lm_weights[w.name]))\n",
    "        else:\n",
    "            print(w.name, 'not initialized')\n",
    "\n",
    "    sess.run(ops)\n",
    "else:\n",
    "    raise ValueError(\"Must specify LM path!\")\n",
    "model = Model(model_name, inp_voc, out_voc, lm, **hp)\n",
    "\n",
    "inp = tf.placeholder(tf.int32, [None, None])\n",
    "out = tf.placeholder(tf.int32, [None, None])\n",
    "logprobs = model.symbolic_score(inp, out, is_train=True)[:,:tf.shape(out)[1]]\n",
    "\n",
    "nll = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logprobs, labels=out)\n",
    "loss = nll * infer_mask(out, out_voc.eos, dtype=tf.float32)\n",
    "loss = tf.reduce_sum(loss, axis=1)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, model_name)\n",
    "\n",
    "all_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "non_trainable_vars = list(set(all_vars).difference(set(weights)))\n",
    "\n",
    "grads = tf.gradients(loss, weights)\n",
    "grads = tf.clip_by_global_norm(grads, 100)[0]\n",
    "optimizer = create_optimizer(hp)\n",
    "train_step = optimizer.apply_gradients(zip(grads, weights))\n",
    "\n",
    "initialize_uninitialized_variables(sess)\n",
    "\n",
    "assigns = []\n",
    "weights_by_common_name = {w.name[len(model_name)+1:]: w for w in weights}\n",
    "\n",
    "with np.load(config.get('target_lm_path')) as dic:\n",
    "    for key in dic: # decoder_init\n",
    "        print(key)\n",
    "        w_lm = dic[key]\n",
    "        weights_key = '/'.join(key.split('/')[1:]).replace('main/','').replace(\"enc\",'dec').replace(\"inp\",\"out\")\n",
    "        if \"emb_out_bias\" in weights_key: # no such thing\n",
    "            continue\n",
    "\n",
    "        w_var = weights_by_common_name[weights_key]\n",
    "\n",
    "        all_shapes_equal(w_lm, w_var, session=sess, mode='assert')\n",
    "\n",
    "        assigns.append(tf.assign(w_var,w_lm))\n",
    "\n",
    "\n",
    "with np.load(config.get(\"src_lm_path\")) as dic:\n",
    "    for key in dic: # encoder_init\n",
    "        w_lm = dic[key]\n",
    "        print(key)\n",
    "        weights_key = '/'.join(key.split('/')[1:]).replace('main/','')\n",
    "        if \"logits\" in weights_key: # encoder has no 'logits' layer for the logits to be initialised\n",
    "            continue\n",
    "        w_var = weights_by_common_name[weights_key]\n",
    "\n",
    "        all_shapes_equal(w_lm, w_var, session=sess, mode='assert')\n",
    "        assigns.append(tf.assign(w_var,w_lm))\n",
    "\n",
    "sess.run(assigns)\n",
    "\n",
    "batch_size = hp.get('batch_size', 32)\n",
    "epoch = 0\n",
    "training_start_time = time()\n",
    "loss_history = []\n",
    "val_scores = []\n",
    "\n",
    "num_iters_done = 0\n",
    "should_start_next_epoch = True # We need this var to break outer loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global num_model_saves_counter; num_model_saves_counter = 0\n",
    "global last_save_epoch; last_save_epoch = -config.get('min_interval_between_saves')\n",
    "\n",
    "def save_model():\n",
    "    global num_model_saves_counter, last_save_epoch\n",
    "\n",
    "    if epoch - config.get('min_interval_between_saves') < last_save_epoch:\n",
    "        print('Do not save the model, because not enough epochs passed')\n",
    "        return\n",
    "\n",
    "    save_path = '{}/model_{}.npz'.format(model_path, num_model_saves_counter % max_num_models)\n",
    "    print('Saving the model into %s' %save_path)\n",
    "\n",
    "    w_values = sess.run(weights)\n",
    "    weights_dict = {w.name: w_val for w, w_val in zip(weights, w_values)}\n",
    "    np.savez(save_path, **weights_dict)\n",
    "\n",
    "    num_model_saves_counter += 1\n",
    "    last_save_epoch = epoch\n",
    "\n",
    "def validate():\n",
    "    \"\"\"\n",
    "    Returns should_continue flag, which tells us if we should continue or early stop\n",
    "    \"\"\"\n",
    "    should_continue = True\n",
    "\n",
    "    if config.get('warm_up_num_epochs') and config.get('warm_up_num_epochs') > epoch:\n",
    "        print('Skipping validation, becaused is not warmed up yet')\n",
    "        return should_continue\n",
    "    else:\n",
    "        print('Cool, I will validate, because warm_up_num_epochs is not set')\n",
    "\n",
    "    print('Validating')\n",
    "    val_score = compute_bleu_for_model(model, sess, inp_voc, out_voc, src_val, dst_val,\n",
    "                                        model_name, config, max_len=max_len)\n",
    "    val_scores.append(val_score)\n",
    "    print('Validation BLEU: {:0.3f}'.format(val_score))\n",
    "\n",
    "    # Save model if this is our best model\n",
    "    if np.argmax(val_scores) == len(val_scores)-1:\n",
    "        print('Saving model because it has the highest validation BLEU.')\n",
    "        save_model()\n",
    "    else:\n",
    "        print('I will not save the model because of its low val_score')\n",
    "\n",
    "    if config.get('use_early_stopping') and should_stop_early(val_scores, config.get('early_stopping_last_n')):\n",
    "        print('Model did not improve for last %s steps. Early stopping.' % config.get('early_stopping_last_n'))\n",
    "        should_continue = False\n",
    "    else:\n",
    "        print('Cool, we will not stop early')\n",
    "\n",
    "    return should_continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[16,91,512]\n\t [[Node: transformer_dec/dec_ffn-0/conv2/add = Add[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](transformer_dec/dec_ffn-0/conv2/Reshape_1, transformer/dec_ffn-0/conv2/b/read)]]\n\t [[Node: Mean/_2403 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_16917_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'transformer_dec/dec_ffn-0/conv2/add', defined at:\n  File \"/home/ubuntu/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-94ab2ce8263d>\", line 21, in <module>\n    logprobs = model.symbolic_score(inp, out, is_train=True)[:,:tf.shape(out)[1]]\n  File \"../models/transformer_fused.py\", line 60, in symbolic_score\n    rdo = self.transformer.decode(out, out_len, out_reverse, enc_out, enc_attn_mask, is_train)\n  File \"../models/transformer_other.py\", line 412, in decode\n    dec_inp = self.dec_ffn[layer](dec_inp)\n  File \"../lib/layers.py\", line 368, in __call__\n    out = self.wrapped_layer(out, *args, **kwargs)\n  File \"../models/transformer_other.py\", line 45, in __call__\n    outputs = self.second_conv(hidden)\n  File \"../lib/layers.py\", line 46, in __call__\n    out = self.activ(dot(inp, self.W) + self.b)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 894, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 183, in add\n    \"Add\", x=x, y=y, name=name)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[16,91,512]\n\t [[Node: transformer_dec/dec_ffn-0/conv2/add = Add[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](transformer_dec/dec_ffn-0/conv2/Reshape_1, transformer/dec_ffn-0/conv2/b/read)]]\n\t [[Node: Mean/_2403 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_16917_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[16,91,512]\n\t [[Node: transformer_dec/dec_ffn-0/conv2/add = Add[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](transformer_dec/dec_ffn-0/conv2/Reshape_1, transformer/dec_ffn-0/conv2/b/read)]]\n\t [[Node: Mean/_2403 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_16917_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c6ca909f7147>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_src_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_dst_ix\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mloss_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[16,91,512]\n\t [[Node: transformer_dec/dec_ffn-0/conv2/add = Add[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](transformer_dec/dec_ffn-0/conv2/Reshape_1, transformer/dec_ffn-0/conv2/b/read)]]\n\t [[Node: Mean/_2403 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_16917_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'transformer_dec/dec_ffn-0/conv2/add', defined at:\n  File \"/home/ubuntu/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-94ab2ce8263d>\", line 21, in <module>\n    logprobs = model.symbolic_score(inp, out, is_train=True)[:,:tf.shape(out)[1]]\n  File \"../models/transformer_fused.py\", line 60, in symbolic_score\n    rdo = self.transformer.decode(out, out_len, out_reverse, enc_out, enc_attn_mask, is_train)\n  File \"../models/transformer_other.py\", line 412, in decode\n    dec_inp = self.dec_ffn[layer](dec_inp)\n  File \"../lib/layers.py\", line 368, in __call__\n    out = self.wrapped_layer(out, *args, **kwargs)\n  File \"../models/transformer_other.py\", line 45, in __call__\n    outputs = self.second_conv(hidden)\n  File \"../lib/layers.py\", line 46, in __call__\n    out = self.activ(dot(inp, self.W) + self.b)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 894, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 183, in add\n    \"Add\", x=x, y=y, name=name)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[16,91,512]\n\t [[Node: transformer_dec/dec_ffn-0/conv2/add = Add[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](transformer_dec/dec_ffn-0/conv2/Reshape_1, transformer/dec_ffn-0/conv2/b/read)]]\n\t [[Node: Mean/_2403 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_16917_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "while should_start_next_epoch:\n",
    "    batches = batch_generator_over_dataset(src_train, dst_train, batch_size, batches_per_epoch=None)\n",
    "    with tqdm(batches) as t:\n",
    "        for batch_src, batch_dst in t:\n",
    "            # Note: we don't use voc.tokenize_many(batch, max_len=max_len)\n",
    "            # cuz it forces batch length to be that long and we often get away with much less\n",
    "            batch_src_ix = inp_voc.tokenize_many(batch_src)[:, :max_len]\n",
    "            batch_dst_ix = out_voc.tokenize_many(batch_dst)[:, :max_len]\n",
    "\n",
    "            feed_dict = {inp: batch_src_ix, out: batch_dst_ix}\n",
    "\n",
    "            loss_t = sess.run([train_step, loss], feed_dict)[1]\n",
    "            loss_history.append(np.mean(loss_t))\n",
    "\n",
    "            loss_hist_val = ewma(np.array(loss_history[-50:]), span=50)[-1]\n",
    "            \n",
    "            if num_iters_done % 10 == 0:\n",
    "                clear_output(True)\n",
    "                plt.figure(figsize=[15,8])\n",
    "                plt.title('Batch loss')\n",
    "                plt.plot(loss_history)\n",
    "                plt.plot(ewma(np.array(loss_history),span=50))\n",
    "                plt.grid()\n",
    "                plt.show()\n",
    "\n",
    "            num_iters_done += 1\n",
    "\n",
    "            if config.get('max_time_seconds'):\n",
    "                seconds_elapsed = time()-training_start_time\n",
    "\n",
    "                if seconds_elapsed > config.get('max_time_seconds'):\n",
    "                    print('Maximum allowed training time reached. Training took %s. Stopping.' % seconds_elapsed)\n",
    "                    should_start_next_epoch = False\n",
    "                    break\n",
    "\n",
    "        if epoch % config.get('validate_every_num_epochs') == 0 and should_start_next_epoch:\n",
    "            should_start_next_epoch = validate()\n",
    "\n",
    "        if config.get('max_epochs') and config.get('max_epochs') == epoch:\n",
    "            print('Maximum amount of epochs reached. Stopping.')\n",
    "            should_start_next_epoch = False\n",
    "            break\n",
    "\n",
    "        epoch +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation scores:\n",
      "[0.005051021781026407, 0.005051021781026407, 0.005051021781026407, 0.005051021781026407, 0.005051021781026407, 0.005051021781026407, 0.005051021781026407]\n",
      "Computing final validation score.\n",
      "Final validation BLEU is: 0.005\n",
      "Saving the model into trained_models/transformer/model_1.npz\n"
     ]
    }
   ],
   "source": [
    "print('Validation scores:')\n",
    "print(val_scores)\n",
    "\n",
    "# Training is done!\n",
    "# Let's check the val score of the model and if it's good â€” save it\n",
    "print('Computing final validation score.')\n",
    "val_score = compute_bleu_for_model(model, sess, inp_voc, out_voc, src_val, dst_val,\n",
    "                                    model_name, config, max_len=max_len)\n",
    "print('Final validation BLEU is: {:0.3f}'.format(val_score))\n",
    "\n",
    "if len(val_scores) == 0 or val_score >= max(val_scores):\n",
    "    save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, we have trained the thing, let's run inference?\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model_name = 'transformer'\n",
    "data_path = '../en-fr-10'\n",
    "\n",
    "config = {\n",
    "    'data_path': data_path,\n",
    "    'models_dir': '../trained_models/transformer',\n",
    "    'input_path': os.path.join(data_path, 'bpe_input.txt'),\n",
    "    'output_path': os.path.join(data_path, 'output.tok.txt'),\n",
    "    'hp_file_path': '../hp_files/trans_default.json',\n",
    "    'batch_size_for_inference': 16,\n",
    "    'target_lm_path': '../trained_models/lm2/model.npz',\n",
    "    'gpu_memory_fraction': 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_path = config.get('input_path')\n",
    "output_path = config.get('output_path')\n",
    "\n",
    "src_data = open(input_path, 'r', encoding='utf-8').read().splitlines()\n",
    "\n",
    "inp_voc = Vocab.from_file('{}/1.voc'.format(config.get('data_path')))\n",
    "out_voc = Vocab.from_file('{}/2.voc'.format(config.get('data_path')))\n",
    "\n",
    "# We get paths to trained models via this argument\n",
    "# We do not save optimizer state, so we can read all files from dir\n",
    "paths_to_models = ['{}/{}'.format(config.get('models_dir'), m) for m in os.listdir(config.get('models_dir'))]\n",
    "print('Found models to ensemble:', paths_to_models)\n",
    "\n",
    "hp = json.load(open(config.get('hp_file_path'), 'r', encoding='utf-8')) if config.get('hp_file_path') else {}\n",
    "gpu_options = create_gpu_options(config)\n",
    "max_len = config.get('max_input_len', 200)\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "lm = TransformerLM('lm2', out_voc, **hp)\n",
    "if config.get('target_lm_path'):\n",
    "    lm_weights = np.load(config.get('target_lm_path'))\n",
    "    ops = []\n",
    "\n",
    "    for w in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, lm.name):\n",
    "        if w.name in lm_weights:\n",
    "            ops.append(tf.assign(w, lm_weights[w.name]))\n",
    "        else:\n",
    "            print(w.name, 'not initialized')\n",
    "\n",
    "    sess.run(ops);\n",
    "else:\n",
    "    raise ValueError(\"Must specify LM path!\")\n",
    "\n",
    "models = []\n",
    "assigns = []\n",
    "print('Loading models')\n",
    "for i, model_path in enumerate(paths_to_models):\n",
    "    print('Loading model from', model_path)\n",
    "    # Loading weights is not an easy task:\n",
    "    # They were saved in transformer/ scope,\n",
    "    # but now we should rename them into transformer_i/ to avoid collision\n",
    "    curr_model_name = 'transformer_' + str(i)\n",
    "    curr_model = Model(curr_model_name, inp_voc, out_voc, lm, **hp)\n",
    "    models.append(curr_model)\n",
    "\n",
    "    curr_weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, curr_model_name)\n",
    "\n",
    "    # Loading current model state\n",
    "    for key, value in np.load(model_path).items():\n",
    "        desired_key = curr_model_name + '/' + '/'.join(key.split('/')[1:]).replace('transformer/', '')\n",
    "        print('Renaming {} => {}'.format(key, desired_key))\n",
    "\n",
    "        # weight_var = tf.get_variable(desired_key)\n",
    "        weight_var = None\n",
    "        for v in curr_weights:\n",
    "            if v.name == desired_key:\n",
    "                weight_var = v\n",
    "                break\n",
    "\n",
    "        assert weight_var != None\n",
    "        assigns.append(tf.assign(weight_var, value))\n",
    "\n",
    "sess.run(assigns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from lib.layers import *\n",
    "from lib.tensor_utils import *\n",
    "from collections import namedtuple\n",
    "from models import TranslateModel\n",
    "\n",
    "\n",
    "class TransformerEnsemble(TranslateModel):\n",
    "    def __init__(self, name, models, inp_voc, out_voc, lm, **hp):\n",
    "        self.name = name\n",
    "        self.inp_voc = inp_voc\n",
    "        self.out_voc = out_voc\n",
    "        self.hp = hp\n",
    "        self.lm = lm\n",
    "        self.debug = hp.get('debug', None)\n",
    "\n",
    "        self.models = models # Here we keep our models\n",
    "        self.DecState = namedtuple(\"transformer_state\", ['state_of_model_%i'%i for i in range(len(models))])\n",
    "\n",
    "    def encode(self, batch, **kwargs):\n",
    "        states = [m.encode(batch, **kwargs) for m in self.models]\n",
    "\n",
    "        return self.DecState(*states)\n",
    "\n",
    "    def decode(self, states, words=None, is_train=False, **kwargs):\n",
    "        states = [m.decode(s, words, is_train, **kwargs) for s, m in zip(states, self.models)]\n",
    "\n",
    "        return self.DecState(*states)\n",
    "\n",
    "    def get_rdo(self, dec_states, **kwargs):\n",
    "        dec_state = dec_states[0]\n",
    "        return dec_state.rdo, dec_state.out_seq\n",
    "\n",
    "    def get_attnP(self, dec_states, **kwargs):\n",
    "        dec_state = dec_states[0]\n",
    "        return dec_state.attnP\n",
    "\n",
    "    def get_logits(self, dec_states, **flags):\n",
    "\n",
    "        logits = []\n",
    "        for state, model in zip(dec_states, self.models):\n",
    "            logits.append(model.get_logits(state, **flags))\n",
    "\n",
    "        return sum(logits) / len(self.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating translations\n",
      "Preparing BEAM SEARCH translate with params: {'swap_memory': True, 'back_prop': False, 'if_no_eos': 'last', 'force_bos': True, 'beam_spread': 3, 'beam_size': 3, 'max_len': None, 'min_len': None, 'batch_placeholder': {'inp': <tf.Tensor 'Placeholder:0' shape=(?, ?) dtype=int32>}, 'self': <lib.inference.BeamSearchInference object at 0x7f94e6b8cda0>, 'model': <__main__.TransformerEnsemble object at 0x7f94f7a0a400>, 'flags': {'len_alpha': 0.9, 'sampling_strategy': 'greedy'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [10:41,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the results into ../en-fr-10/output.tok.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ensemble = TransformerEnsemble(model_name, models, inp_voc, out_voc, lm, **hp)\n",
    "initialize_uninitialized_variables(sess)\n",
    "\n",
    "print('Generating translations')\n",
    "inp = tf.placeholder(tf.int32, [None, None])\n",
    "sy_translations = ensemble.symbolic_translate(inp, back_prop=False, swap_memory=True).best_out\n",
    "translations = []\n",
    "\n",
    "for batch in tqdm(iterate_minibatches(src_data, batchsize=config.get('batch_size_for_inference'))):\n",
    "    try:\n",
    "        batch_data_ix = inp_voc.tokenize_many(batch[0])[:, :max_len]\n",
    "        trans_ix = sess.run([sy_translations], feed_dict={inp: batch_data_ix})[0]\n",
    "        # deprocess = True gets rid of BOS and EOS\n",
    "        trans = out_voc.detokenize_many(trans_ix, unbpe=True, deprocess=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        # we failed this batch. At least one sample is broken\n",
    "        trans = []\n",
    "        src_rows = batch[0]\n",
    "        for row in src_rows:\n",
    "            try:\n",
    "                row_ix = inp_voc.tokenize_many([row])[:, :max_len]  # [1, inp_len]\n",
    "\n",
    "                row_trans_ix = sess.run([sy_translations],\n",
    "                                        feed_dict={inp: row_ix})[0]  # [1, out_len]\n",
    "\n",
    "                # deprocess = True gets rid of BOS and EOS\n",
    "                row_trans = out_voc.detokenize_many(row_trans_ix,\n",
    "                                                    unbpe=True, deprocess=True)  # [1]\n",
    "                trans.append(row_trans[0])\n",
    "            except Exception as e:\n",
    "                # we failed this very row. Use src as fallback\n",
    "                trans.append(str(row).replace('\\n', ''))  # cast to str just in case\n",
    "\n",
    "    translations.extend(trans)\n",
    "\n",
    "print('Saving the results into %s' % output_path)\n",
    "\n",
    "with open(output_path, 'wb') as output_file:\n",
    "    output_file.write('\\n'.join(translations).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/universome/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Tokenizing\n",
      "BLEU: 0.0032\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from bleu import compute_bleu\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "targets = open(os.path.join(data_path, 'ground_truth.txt'), 'r', encoding='utf-8').read().splitlines()\n",
    "\n",
    "print('Tokenizing')\n",
    "tokenized_refs = [[word_tokenize(t)] for t in targets]\n",
    "tokenized_tranlations = [word_tokenize(s) for s in translations]\n",
    "\n",
    "bleu = compute_bleu(tokenized_refs, tokenized_tranlations)[0]\n",
    "print('BLEU: {:.4f}'.format(bleu))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
