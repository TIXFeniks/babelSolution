{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from random import choice\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import ewma\n",
    "\n",
    "from vocab import Vocab\n",
    "from src.training_utils import *\n",
    "from lib.tensor_utils import infer_mask, initialize_uninitialized_variables, all_shapes_equal\n",
    "from lib.utils import save_score\n",
    "\n",
    "from models.transformer_fused import Model\n",
    "from models.transformer_lm import TransformerLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "model_name = 'transformer'\n",
    "\n",
    "config = {\n",
    "    'data_path': '../data_small',\n",
    "    'src_lm_path': '../trained_models/lm1/model.npz',\n",
    "    'target_lm_path': '../trained_models/lm2/model.npz',\n",
    "    'hp_file_path': '../hp_files/trans_default.json',\n",
    "    'use_early_stopping': True,\n",
    "    'early_stopping_last_n': 10,\n",
    "    'max_epochs': 1000,\n",
    "    'max_time_seconds': 600,\n",
    "    'batch_size_for_inference': 16,\n",
    "    'max_len': 200,\n",
    "    'validate_every_epoch': True,\n",
    "    'warm_up_num_epochs': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = 'trained_models/{}'.format(model_name)\n",
    "if not os.path.isdir('trained_models'): os.mkdir('trained_models')\n",
    "if not os.path.isdir(model_path): os.mkdir(model_path)\n",
    "\n",
    "src_train_path = '{}/bpe_parallel_train1.txt'.format(config.get('data_path'))\n",
    "dst_train_path = '{}/bpe_parallel_train2.txt'.format(config.get('data_path'))\n",
    "src_val_path = '{}/bpe_parallel_val1.txt'.format(config.get('data_path'))\n",
    "dst_val_path = '{}/bpe_parallel_val2.txt'.format(config.get('data_path'))\n",
    "src_unlabeled_path = '{}/bpe_corpus1.txt'.format(config.get('data_path'))\n",
    "dst_unlabeled_path = '{}/bpe_corpus2.txt'.format(config.get('data_path'))\n",
    "\n",
    "src_train = open(src_train_path, 'r', encoding='utf-8').read().splitlines()\n",
    "dst_train = open(dst_train_path, 'r', encoding='utf-8').read().splitlines()\n",
    "src_val = open(src_val_path, 'r', encoding='utf-8').read().splitlines()\n",
    "dst_val = open(dst_val_path, 'r', encoding='utf-8').read().splitlines()\n",
    "src_unlabeled = open(src_unlabeled_path, 'r', encoding='utf-8').read().splitlines()\n",
    "dst_unlabeled = open(dst_unlabeled_path, 'r', encoding='utf-8').read().splitlines()\n",
    "\n",
    "inp_voc = Vocab.from_file('{}/1.voc'.format(config.get('data_path')))\n",
    "out_voc = Vocab.from_file('{}/2.voc'.format(config.get('data_path')))\n",
    "max_len = config.get('max_len', 200)\n",
    "\n",
    "# Hyperparameters\n",
    "hp = json.load(open(config.get('hp_file_path'), 'r', encoding='utf-8')) if config.get('hp_file_path') else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_options = create_gpu_options(config)\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "# Disabling GPU\n",
    "# tf_config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "# sess = tf.InteractiveSession(config=tf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = create_optimizer(hp)\n",
    "inp = tf.placeholder(tf.int32, [None, None])\n",
    "out = tf.placeholder(tf.int32, [None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "### Initializing our main model\n",
    "#############################################\n",
    "lm = TransformerLM('lm2', out_voc, **hp)\n",
    "if config.get('target_lm_path'):\n",
    "    lm_weights = np.load(config.get('target_lm_path'))\n",
    "    ops = []\n",
    "    for w in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, lm.name):\n",
    "        if w.name in lm_weights:\n",
    "            ops.append(tf.assign(w, lm_weights[w.name]))\n",
    "        else:\n",
    "            print(w.name, 'not initialized')\n",
    "    sess.run(ops)\n",
    "else:\n",
    "    raise ValueError(\"Must specify LM path!\")\n",
    "\n",
    "model = Model(model_name, inp_voc, out_voc, lm, **hp)\n",
    "\n",
    "logprobs = model.symbolic_score(inp, out, is_train=True)[:,:tf.shape(out)[1]]\n",
    "nll = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logprobs, labels=out)\n",
    "loss = nll * infer_mask(out, out_voc.eos, dtype=tf.float32)\n",
    "loss = tf.reduce_sum(loss, axis=1)\n",
    "loss = tf.reduce_mean(loss)\n",
    "weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, model_name)\n",
    "\n",
    "grads = tf.gradients(loss, weights)\n",
    "grads = tf.clip_by_global_norm(grads, 100)[0]\n",
    "train_step = optimizer.apply_gradients(zip(grads, weights))\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "### Initializing our back-translation model\n",
    "### TODO(universome): DRY\n",
    "#############################################\n",
    "lm_bk = TransformerLM('lm1', inp_voc, **hp)\n",
    "if config.get('src_lm_path'):\n",
    "    lm_weights = np.load(config.get('src_lm_path'))\n",
    "    ops = []\n",
    "    for w in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, lm_bk.name):\n",
    "        if w.name in lm_weights:\n",
    "            ops.append(tf.assign(w, lm_weights[w.name]))\n",
    "        else:\n",
    "            print(w.name, 'not initialized')\n",
    "    sess.run(ops)\n",
    "else:\n",
    "    raise ValueError(\"Must specify src LM path!\")\n",
    "model_bk = Model(model_name + \"bk\", out_voc, inp_voc, lm_bk, **hp)\n",
    "\n",
    "logprobs_bk = model_bk.symbolic_score(out, inp, is_train=True)[:, :tf.shape(inp)[1]]\n",
    "nll_bk = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logprobs_bk, labels=inp)\n",
    "loss_bk = nll_bk * infer_mask(inp, inp_voc.eos, dtype=tf.float32)\n",
    "loss_bk = tf.reduce_sum(loss_bk, axis=1)\n",
    "loss_bk = tf.reduce_mean(loss_bk)\n",
    "weights_bk = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, model_name + 'bk')\n",
    "\n",
    "grads_bk = tf.gradients(loss_bk, weights_bk)\n",
    "grads_bk = tf.clip_by_global_norm(grads_bk, 100)[0]\n",
    "train_step_bk = optimizer.apply_gradients(zip(grads_bk, weights_bk))\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "non_trainable_vars = list(set(all_vars).difference(set(weights + weights_bk)))\n",
    "\n",
    "initialize_uninitialized_variables(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_with_lms(model, weights, target_lm_path, src_lm_path, session):\n",
    "    assigns = []\n",
    "    # init model with LMs\n",
    "    weights_by_common_name = {w.name[len(model.name) + 1:]: w for w in weights}\n",
    "    with np.load(target_lm_path) as dic:\n",
    "        for key in dic:  # decoder_init\n",
    "            w_lm = dic[key]\n",
    "            weights_key = '/'.join(key.split('/')[1:]).replace('main/', '').replace(\"enc\", 'dec').replace(\"inp\", \"out\")\n",
    "            if \"emb_out_bias\" in weights_key:  # no such thing\n",
    "                continue\n",
    "\n",
    "            w_var = weights_by_common_name[weights_key]\n",
    "\n",
    "            all_shapes_equal(w_lm, w_var, session=session, mode='assert')\n",
    "\n",
    "            assigns.append(tf.assign(w_var, w_lm))\n",
    "    with np.load(src_lm_path) as dic:\n",
    "        for key in dic:  # encoder_init\n",
    "            w_lm = dic[key]\n",
    "            weights_key = '/'.join(key.split('/')[1:]).replace('main/', '')\n",
    "            if \"logits\" in weights_key:  # encoder has no 'logits' layer for the logits to be initialised\n",
    "                continue\n",
    "            w_var = weights_by_common_name[weights_key]\n",
    "\n",
    "            all_shapes_equal(w_lm, w_var, session=session, mode='assert')\n",
    "            assigns.append(tf.assign(w_var, w_lm))\n",
    "    session.run(assigns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert config.get('target_lm_path') and config.get('src_lm_path')\n",
    "init_with_lms(model, weights, config.get('target_lm_path'), config.get('src_lm_path'), sess)\n",
    "init_with_lms(model_bk, weights_bk, config.get('src_lm_path'), config.get('target_lm_path'), sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = hp.get('batch_size', 32)\n",
    "epoch = 0\n",
    "training_start_time = time()\n",
    "loss_history = []\n",
    "loss_history_bk = []\n",
    "val_scores = []\n",
    "\n",
    "num_iters_done = 0\n",
    "should_start_next_epoch = True # We need this var to break outer loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    save_path = '{}/model.npz'.format(model_path)\n",
    "    print('Saving the model into %s' %save_path)\n",
    "\n",
    "    w_values = sess.run(weights)\n",
    "    weights_dict = {w.name: w_val for w, w_val in zip(weights, w_values)}\n",
    "    np.savez(save_path, **weights_dict)\n",
    "\n",
    "def validate(val_scores):\n",
    "    \"\"\"\n",
    "    Returns should_continue flag, which tells us if we should continue or early stop\n",
    "    \"\"\"\n",
    "    should_continue = True\n",
    "\n",
    "    # if config.get('warm_up_num_epochs') and config.get('warm_up_num_epochs') > epoch:\n",
    "    #     print('Skipping validation, becaused is not warmed up yet')\n",
    "    #     return should_continue\n",
    "\n",
    "    print('Validating')\n",
    "    val_score = compute_bleu_for_model(model, sess, inp_voc, out_voc, src_val, dst_val,\n",
    "                                        model_name, config, max_len=max_len)\n",
    "    val_scores.append(val_score)\n",
    "    print('Validation BLEU: {:0.3f}'.format(val_score))\n",
    "\n",
    "    # Save model if this is our best model\n",
    "    if np.argmax(val_scores) == len(val_scores)-1:\n",
    "        print('Saving model because it has the highest validation BLEU.')\n",
    "        save_model()\n",
    "\n",
    "    if config.get('use_early_stopping') and should_stop_early(val_scores, config.get('early_stopping_last_n')):\n",
    "        print('Model did not improve for last %s steps. Early stopping.' % config.get('early_stopping_last_n'))\n",
    "        should_continue = False\n",
    "\n",
    "    if config.get('warm_up_num_epochs') > epoch:\n",
    "        return True\n",
    "    else:\n",
    "        return should_continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syntethic_src_trg = []\n",
    "syntethic_trg_src = []\n",
    "max_num_synthetic_sents = config.get('max_num_synthetic_sents', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate_sents(model, sess, inp_voc, out_voc, src_val, model_name, config, max_len=200):\n",
    "    src_val_ix = inp_voc.tokenize_many(src_val)\n",
    "\n",
    "    inp = tf.placeholder(tf.int32, [None, None])\n",
    "    translations = []\n",
    "\n",
    "    if model_name == 'gnmt':\n",
    "        raise NotImplemented(\"deprecated model\")\n",
    "    sy_translations = model.symbolic_translate(inp, mode='greedy', max_len=max_len,\n",
    "                                               back_prop=False, swap_memory=True).best_out\n",
    "\n",
    "    for batch in iterate_minibatches(src_val_ix, batchsize=config.get('batch_size_for_inference', 64)):\n",
    "        translations += sess.run([sy_translations], feed_dict={inp: batch[0][:, :max_len]})[0].tolist()\n",
    "\n",
    "    outputs = out_voc.detokenize_many(translations, unbpe=True, deprocess=True)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:56: FutureWarning: pd.ewm_mean is deprecated for ndarrays and will be removed in a future version\n",
      "Iterations done: 1. Loss: 145.76: : 2it [00:04,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n",
      "Validation BLEU: 0.000\n",
      "Saving model because it has the highest validation BLEU.\n",
      "Saving the model into trained_models/transformer/model.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'syntethic_src_dst_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-29388f2b623f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m                                            for i in range(batch_size // 10 + 1)]\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mbatch_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msyntethic_trg_src\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msyntethic_src_dst_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0mbatch_dst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msyntethic_trg_src\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msyntethic_src_dst_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'syntethic_src_dst_batch' is not defined"
     ]
    }
   ],
   "source": [
    "should_start_next_epoch = True\n",
    "\n",
    "while should_start_next_epoch:\n",
    "    batches = batch_generator_over_dataset(src_train, dst_train, batch_size, batches_per_epoch=None)\n",
    "    with tqdm(batches) as t:\n",
    "        for batch_src, batch_dst in t:\n",
    "            # Note: we don't use voc.tokenize_many(batch, max_len=max_len)\n",
    "            # cuz it forces batch length to be that long and we often get away with much less\n",
    "            batch_src_ix = inp_voc.tokenize_many(batch_src)[:, :max_len]\n",
    "            batch_dst_ix = out_voc.tokenize_many(batch_dst)[:, :max_len]\n",
    "\n",
    "            feed_dict = {inp: batch_src_ix, out: batch_dst_ix}\n",
    "            _, loss_t, _, loss_t_bk = sess.run([train_step, loss, train_step_bk, loss_bk], feed_dict)\n",
    "\n",
    "            loss_history.append(np.mean(loss_t))\n",
    "            loss_history_bk.append(np.mean(loss_t_bk))\n",
    "\n",
    "            if len(syntethic_src_trg) > 0:\n",
    "                # src -> dst back trans results\n",
    "                syntethic_src_trg_batch = [choice(range(len(syntethic_src_trg)))\n",
    "                                           for i in range(batch_size // 10 + 1)]\n",
    "\n",
    "                batch_src = [syntethic_src_trg[i][0] for i in syntethic_src_trg_batch]\n",
    "                batch_dst = [syntethic_src_trg[i][1] for i in syntethic_src_trg_batch]\n",
    "\n",
    "                batch_src_ix = inp_voc.tokenize_many(batch_src)[:, :max_len]\n",
    "                batch_dst_ix = out_voc.tokenize_many(batch_dst)[:, :max_len]\n",
    "\n",
    "                feed_dict = {inp: batch_src_ix, out: batch_dst_ix}\n",
    "                _, loss_t_bk = sess.run([train_step_bk, loss_bk], feed_dict)\n",
    "\n",
    "            if len(syntethic_trg_src) > 0:\n",
    "                # src <- dst back trans results\n",
    "                syntethic_trg_src_batch = [choice(range(len(syntethic_trg_src)))\n",
    "                                           for i in range(batch_size // 10 + 1)]\n",
    "\n",
    "                batch_src = [syntethic_trg_src[i][1] for i in syntethic_trg_src_batch]\n",
    "                batch_dst = [syntethic_trg_src[i][0] for i in syntethic_trg_src_batch]\n",
    "\n",
    "                batch_src_ix = inp_voc.tokenize_many(batch_src)[:, :max_len]\n",
    "                batch_dst_ix = out_voc.tokenize_many(batch_dst)[:, :max_len]\n",
    "\n",
    "                feed_dict = {inp: batch_src_ix, out: batch_dst_ix}\n",
    "                _, loss_t = sess.run([train_step, loss], feed_dict)\n",
    "\n",
    "            # Note: we don't use voc.tokenize_many(batch, max_len=max_len)\n",
    "            # cuz it forces batch length to be that long and we often get away with much less\n",
    "            batch_src_ix = inp_voc.tokenize_many(batch_src)[:, :max_len]\n",
    "            batch_dst_ix = out_voc.tokenize_many(batch_dst)[:, :max_len]\n",
    "\n",
    "            feed_dict = {inp: batch_src_ix, out: batch_dst_ix}\n",
    "\n",
    "            loss_t = sess.run([train_step, loss], feed_dict)[1]\n",
    "            loss_history.append(np.mean(loss_t))\n",
    "\n",
    "            loss_to_print = ewma(np.array(loss_history[-50:]), span=50)[-1]\n",
    "            t.set_description('Iterations done: {}. Loss: {:.2f}'.format(num_iters_done, loss_to_print))\n",
    "\n",
    "            if not config.get('validate_every_epoch') and (num_iters_done+1) % config.get('validate_every', 500) == 0:\n",
    "                should_continue = validate(val_scores)\n",
    "                if not should_continue:\n",
    "                    should_start_next_epoch = False\n",
    "                    break\n",
    "\n",
    "            num_iters_done += 1\n",
    "\n",
    "            if config.get('max_time_seconds'):\n",
    "                seconds_elapsed = time()-training_start_time\n",
    "\n",
    "                if seconds_elapsed > config.get('max_time_seconds'):\n",
    "                    print('Maximum allowed training time reached. Training took %s. Stopping.' % seconds_elapsed)\n",
    "                    should_start_next_epoch = False\n",
    "                    break\n",
    "\n",
    "        epoch +=1\n",
    "\n",
    "        if config.get('validate_every_epoch') and should_start_next_epoch:\n",
    "            should_start_next_epoch = validate(val_scores)\n",
    "\n",
    "        if config.get('max_epochs') and config.get('max_epochs') == epoch:\n",
    "            print('Maximum amount of epochs reached. Stopping.')\n",
    "            break\n",
    "\n",
    "        src_to_trans = [choice(src_unlabeled) for i in range(config.get('synthetic_per_epoch', 100))]\n",
    "        dst_to_trans = [choice(dst_unlabeled) for i in range(config.get('synthetic_per_epoch', 100))]\n",
    "        syntethic_src_trg += zip(src_to_trans, translate_sents(model, sess, inp_voc, out_voc,\n",
    "                                                            src_to_trans, model_name, config))\n",
    "        syntethic_trg_src += zip(dst_to_trans, translate_sents(model_bk, sess, out_voc, inp_voc,\n",
    "                                                            dst_to_trans, model_name + 'bk', config))\n",
    "\n",
    "        syntethic_trg_src = syntethic_trg_src[-max_num_synthetic_sents:]\n",
    "        syntethic_src_trg = syntethic_src_trg[-max_num_synthetic_sents:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
