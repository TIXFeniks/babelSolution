{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import collections\n",
    "import math\n",
    "\n",
    "\n",
    "def _get_ngrams(segment, max_order):\n",
    "    \"\"\"Extracts all n-grams upto a given maximum order from an input segment.\n",
    "    Args:\n",
    "      segment: text segment from which n-grams will be extracted.\n",
    "      max_order: maximum length in tokens of the n-grams returned by this\n",
    "          methods.\n",
    "    Returns:\n",
    "      The Counter containing all n-grams upto max_order in segment\n",
    "      with a count of how many times each n-gram occurred.\n",
    "    \"\"\"\n",
    "    ngram_counts = collections.Counter()\n",
    "    for order in range(1, max_order + 1):\n",
    "        for i in range(0, len(segment) - order + 1):\n",
    "            ngram = tuple(segment[i:i + order])\n",
    "            ngram_counts[ngram] += 1\n",
    "    return ngram_counts\n",
    "\n",
    "\n",
    "def compute_bleu(reference_corpus, translation_corpus, max_order=4,\n",
    "                 smooth=False):\n",
    "    \"\"\"Computes BLEU score of translated segments against one or more references.\n",
    "    Args:\n",
    "      reference_corpus: list of lists of references for each translation. Each\n",
    "          reference should be tokenized into a list of tokens.\n",
    "      translation_corpus: list of translations to score. Each translation\n",
    "          should be tokenized into a list of tokens.\n",
    "      max_order: Maximum n-gram order to use when computing BLEU score.\n",
    "      smooth: Whether or not to apply Lin et al. 2004 smoothing.\n",
    "    Returns:\n",
    "      3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n",
    "      precisions and brevity penalty.\n",
    "    \"\"\"\n",
    "    matches_by_order = [0] * max_order\n",
    "    possible_matches_by_order = [0] * max_order\n",
    "    reference_length = 0\n",
    "    translation_length = 0\n",
    "    for (references, translation) in zip(reference_corpus,\n",
    "                                         translation_corpus):\n",
    "        reference_length += min(len(r) for r in references)\n",
    "        translation_length += len(translation)\n",
    "\n",
    "        merged_ref_ngram_counts = collections.Counter()\n",
    "        for reference in references:\n",
    "            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n",
    "        translation_ngram_counts = _get_ngrams(translation, max_order)\n",
    "        overlap = translation_ngram_counts & merged_ref_ngram_counts\n",
    "        for ngram in overlap:\n",
    "            matches_by_order[len(ngram) - 1] += overlap[ngram]\n",
    "        for order in range(1, max_order + 1):\n",
    "            possible_matches = len(translation) - order + 1\n",
    "            if possible_matches > 0:\n",
    "                possible_matches_by_order[order - 1] += possible_matches\n",
    "\n",
    "    precisions = [0] * max_order\n",
    "    for i in range(0, max_order):\n",
    "        if smooth:\n",
    "            precisions[i] = ((matches_by_order[i] + 1.) /\n",
    "                             (possible_matches_by_order[i] + 1.))\n",
    "        else:\n",
    "            if possible_matches_by_order[i] > 0:\n",
    "                precisions[i] = (float(matches_by_order[i]) /\n",
    "                                 possible_matches_by_order[i])\n",
    "            else:\n",
    "                precisions[i] = 0.0\n",
    "\n",
    "    if min(precisions) > 0:\n",
    "        p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n",
    "        geo_mean = math.exp(p_log_sum)\n",
    "    else:\n",
    "        geo_mean = 0\n",
    "\n",
    "    ratio = (float(translation_length) / reference_length) if reference_length > 0 else 0.0\n",
    "\n",
    "    if ratio > 1.0:\n",
    "        bp = 1.\n",
    "    else:\n",
    "        bp = math.exp(1 - 1. / ratio) if ratio > 0 else 0.0\n",
    "\n",
    "    bleu = geo_mean * bp\n",
    "\n",
    "    return (bleu, precisions, bp, ratio, translation_length, reference_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_path = \"../outputs_en_ru//\"\n",
    "path = pre_path + \"20K/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['init_fuse.txt', 'init_no_fuse.txt', 'no_init_no_fuse.txt']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pre_path + \"output.txt\") as f:\n",
    "    ref_lines = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref = [[word_tokenize(x)] for x in ref_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_fuse.txt 0.2715294747336381\n",
      "init_no_fuse.txt 0.27727729881363755\n",
      "no_init_no_fuse.txt 0.26330739221145333\n"
     ]
    }
   ],
   "source": [
    "for f_name in files:\n",
    "    with open(path + f_name) as f:\n",
    "        target_lines = f.read().splitlines()\n",
    "       \n",
    "    target = [word_tokenize(x) for x in target_lines]\n",
    "    \n",
    "    print(f_name, compute_bleu(ref[:], target[:])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_no_fuse Enfin , je voudrais remercier le commissaire d' avoir fait preuve de beaucoup d' ouverture à nos propositions .\n",
      "fuse_no_init Enfin , je remercie M . le commissaire pour l' être très ouvert à nos propositions .\n",
      "no_fuse_no_init Pour terminer , je voudrais remercier le commissaire d' être très ouvert à nos propositions .\n",
      "fuse_init Enfin , je voudrais remercier le commissaire pour l' être très ouverte à nos propositions .\n",
      "target Enfin , je voudrais saluer l ' excellente disposition dont la commissaire fait montre concernant nos propositions .\n"
     ]
    }
   ],
   "source": [
    "for name, f_name in zip(names, files):\n",
    "    with open(\"./\" + f_name) as f:\n",
    "        target_lines = f.read().splitlines()\n",
    "       \n",
    "    target = [word_tokenize(x) for x in target_lines]\n",
    "    \n",
    "    print(name, target_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1408822747028321,\n",
       " [0.4967637540453074,\n",
       "  0.22073578595317725,\n",
       "  0.11072664359861592,\n",
       "  0.05734767025089606],\n",
       " 0.867278768135988,\n",
       " 0.8753541076487252,\n",
       " 618,\n",
       " 706)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_bleu(ref[:], target[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Enfin , je voudrais saluer l ' excellente disposition dont la commissaire fait montre concernant nos propositions .\",\n",
       " \"Monsieur le Président , Madame la Commissaire , chers collègues , le chiffre catastrophique de 42 000 morts sur les routes de l ' Union européenne chaque année , les blessés , dont le nombre s ' élève à 1 , 7 million , nous incitent constamment à considérer le thème de la sécurité des transports comme un problème très important et prioritaire .\",\n",
       " \"Ces chiffres cachent non seulement une souffrance personnelle incommensurable , mais ils impliquent aussi d ' énormes pertes économiques , et donc d ' énormes coûts .\",\n",
       " \"Les estimations se chiffrent à 100 milliards d ' euros , ce qui correspond à environ 2 % du produit intérieur brut de tous les États membres européens .\",\n",
       " \"C ' est plus que ce que tous les États membres de l ' Union européenne réunis dépensent pour la culture .\"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Enfin , je remercie M . le commissaire pour l' être très ouvert à nos propositions .\",\n",
       " \"Monsieur le Président , Monsieur le Commissaire , chers collègues , le chiffre effroyable de 42 , 000 à un an sur les routes de l' Union européenne , on estime que 1 , 7 millions de livres , c' est - à - dire que nous devons accorder la priorité à la sécurité routière .\",\n",
       " \"Ces chiffres ne constituent pas seulement des grands critères , bien qu' ils constituent également des sujets économiques et de coûts .\",\n",
       " \"Ceux - ci estime qu' à 100 milliards d' euros , la raison pour certains 2 % du produit intérieur brut de l' ensemble des États membres européens .\",\n",
       " \"C' est plus que tous les États membres de l' Union européenne ont mis un terme à leur culture .\"]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Enfin',\n",
       " ',',\n",
       " 'je',\n",
       " 'voudrais',\n",
       " 'saluer',\n",
       " 'l',\n",
       " \"'\",\n",
       " 'excellente',\n",
       " 'disposition',\n",
       " 'dont',\n",
       " 'la',\n",
       " 'commissaire',\n",
       " 'fait',\n",
       " 'montre',\n",
       " 'concernant',\n",
       " 'nos',\n",
       " 'propositions',\n",
       " '.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
