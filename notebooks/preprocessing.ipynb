{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "perl: warning: Setting locale failed.\n",
      "perl: warning: Please check that your locale settings:\n",
      "\tLANGUAGE = (unset),\n",
      "\tLC_ALL = (unset),\n",
      "\tLC_CTYPE = \"UTF-8\",\n",
      "\tLANG = \"en_US.UTF-8\"\n",
      "    are supported and installed on your system.\n",
      "perl: warning: Falling back to a fallback locale (\"en_US.UTF-8\").\n",
      "perl: warning: Setting locale failed.\n",
      "perl: warning: Please check that your locale settings:\n",
      "\tLANGUAGE = (unset),\n",
      "\tLC_ALL = (unset),\n",
      "\tLC_CTYPE = \"UTF-8\",\n",
      "\tLANG = \"en_US.UTF-8\"\n",
      "    are supported and installed on your system.\n",
      "perl: warning: Falling back to a fallback locale (\"en_US.UTF-8\").\n",
      "perl: warning: Setting locale failed.\n",
      "perl: warning: Please check that your locale settings:\n",
      "\tLANGUAGE = (unset),\n",
      "\tLC_ALL = (unset),\n",
      "\tLC_CTYPE = \"UTF-8\",\n",
      "\tLANG = \"en_US.UTF-8\"\n",
      "    are supported and installed on your system.\n",
      "perl: warning: Falling back to a fallback locale (\"en_US.UTF-8\").\n",
      "Tokenizer Version 1.1\n",
      "Language: en\n",
      "Number of threads: 4\n",
      "perl: warning: Setting locale failed.\n",
      "perl: warning: Please check that your locale settings:\n",
      "\tLANGUAGE = (unset),\n",
      "\tLC_ALL = (unset),\n",
      "\tLC_CTYPE = \"UTF-8\",\n",
      "\tLANG = \"en_US.UTF-8\"\n",
      "    are supported and installed on your system.\n",
      "perl: warning: Falling back to a fallback locale (\"en_US.UTF-8\").\n",
      "perl: warning: Setting locale failed.\n",
      "perl: warning: Please check that your locale settings:\n",
      "\tLANGUAGE = (unset),\n",
      "\tLC_ALL = (unset),\n",
      "\tLC_CTYPE = \"UTF-8\",\n",
      "\tLANG = \"en_US.UTF-8\"\n",
      "    are supported and installed on your system.\n",
      "perl: warning: Falling back to a fallback locale (\"en_US.UTF-8\").\n",
      "perl: warning: Setting locale failed.\n",
      "perl: warning: Please check that your locale settings:\n",
      "\tLANGUAGE = (unset),\n",
      "\tLC_ALL = (unset),\n",
      "\tLC_CTYPE = \"UTF-8\",\n",
      "\tLANG = \"en_US.UTF-8\"\n",
      "    are supported and installed on your system.\n",
      "perl: warning: Falling back to a fallback locale (\"en_US.UTF-8\").\n",
      "Tokenizer Version 1.1\n",
      "Language: en\n",
      "Number of threads: 4\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "home=\"..\"\n",
    "data=\"$home/data\"\n",
    "data_generated=\"$data/generated\"\n",
    "threads=4\n",
    "mosesdecoder=\"$home/ext-libs/mosesdecoder\"\n",
    "subword_nmt=\"$home/ext-libs/subword-nmt\"\n",
    "\n",
    "src_raw=\"$data/OpenSubtitles2018.fa-tr.fa\"\n",
    "trg_raw=\"$data/OpenSubtitles2018.fa-tr.tr\"\n",
    "\n",
    "mkdir \"$data_generated\" # Here we'll keep all our files\n",
    "\n",
    "# First things first: tokenization\n",
    "cat $src_raw | \\\n",
    "    $mosesdecoder/scripts/tokenizer/normalize-punctuation.perl | \\\n",
    "    $mosesdecoder/scripts/tokenizer/tokenizer.perl -threads $threads > \\\n",
    "    $data_generated/src.tok\n",
    "    \n",
    "cat $trg_raw | \\\n",
    "    $mosesdecoder/scripts/tokenizer/normalize-punctuation.perl | \\\n",
    "    $mosesdecoder/scripts/tokenizer/tokenizer.perl -threads $threads > \\\n",
    "    $data_generated/trg.tok\n",
    "    \n",
    "\n",
    "# Second things second: learning BPEs\n",
    "num_tokens=8000\n",
    "\n",
    "for domain in src trg\n",
    "do\n",
    "$subword_nmt/learn_joint_bpe_and_vocab.py \\\n",
    "    -i $data_generated/$domain.tok -s $num_tokens -o $data_generated/$domain.bpe \\\n",
    "    --write-vocabulary $data_generated/vocab.$domain\n",
    "done\n",
    "    \n",
    "\n",
    "# Third things third: applying BPEs\n",
    "for domain in src trg\n",
    "do\n",
    "cat $data_generated/$domain.tok | $subword_nmt/apply_bpe.py -c $data_generated/$domain.bpe \\\n",
    "    --vocabulary $data_generated/vocab.$domain --vocabulary-threshold 0 \\\n",
    "    -o $data_generated/$domain.tok.bpe\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "model_src = fasttext.skipgram('../data/generated/src.tok.bpe',\n",
    "                              '../trained_models/src.tok.bpe_cbow',\n",
    "                              dim=512, min_count=1, silent=0, thread=4)\n",
    "\n",
    "model_src = fasttext.skipgram('../data/generated/trg.tok.bpe',\n",
    "                              '../trained_models/trg.tok.bpe_cbow',\n",
    "                              dim=512, min_count=1, silent=0, thread=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split our datasets into parallel and unparallel corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# print('Reading datasets')\n",
    "# src_raw = open('../data/OpenSubtitles2018.fa-tr.fa', encoding='utf-8').read().splitlines()\n",
    "# trg_raw = open('../data/OpenSubtitles2018.fa-tr.tr', encoding='utf-8').read().splitlines()\n",
    "\n",
    "# # Let's remove sentences which are suspiciously long or short\n",
    "# # and which have two much length ratio between source/target\n",
    "# max_len = 200\n",
    "# max_ratio = 3\n",
    "\n",
    "# lines_to_remove = set()\n",
    "# bad_ratio_lines = set()\n",
    "# bad_min_len_lines = set()\n",
    "# bad_max_len_lines = set()\n",
    "\n",
    "# print('Searching for bad lines')\n",
    "# for i in tqdm(range(len(src_raw))):\n",
    "#     src_len = len(src_raw[i].split())\n",
    "#     trg_len = len(trg_raw[i].split())\n",
    "    \n",
    "#     lens_are_ok = src_len <= max_len and trg_len <= max_len\n",
    "#     ratios_are_ok = (1 / max_ratio) <= src_len / trg_len <= max_ratio\n",
    "    \n",
    "#     if not lens_are_ok or not ratios_are_ok: lines_to_remove.add(i)\n",
    "        \n",
    "# print('Num lines to remove:', len(lines_to_remove))\n",
    "# src = [line for i, line in enumerate(src_raw) if not i in lines_to_remove]\n",
    "# trg = [line for i, line in enumerate(trg_raw) if not i in lines_to_remove]\n",
    "# print('Num lines left:', len(src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Now, we can split data into datasets\n",
    "num_parallel = 10000\n",
    "num_test = 1000\n",
    "num_val = 1000\n",
    "\n",
    "data = {}\n",
    "\n",
    "split = train_test_split(src, trg, test_size=num_parallel, random_state=42)\n",
    "data['src_mono'], data['src_parallel'], data['trg_mono'], data['trg_parallel'] = split\n",
    "\n",
    "# Parallel corpus we should additionally split into train/val/test.\n",
    "split = train_test_split(data['src_parallel'], data['trg_parallel'], test_size=num_test, random_state=42)\n",
    "data['src_train'], data['src_test'], data['trg_train'], data['trg_test'] = split\n",
    "\n",
    "split = train_test_split(data['src_train'], data['trg_train'], test_size=num_val, random_state=42)\n",
    "data['src_train'], data['src_val'], data['trg_train'], data['trg_val'] = split\n",
    "\n",
    "# Saving the results\n",
    "for dataset_name in data:\n",
    "    file_path = '../data/generated/{}.tok.bpe'.format(dataset_name)\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for line in data[dataset_name]:\n",
    "            f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "lens_src = [len(s.split()) for s in src]\n",
    "lens_trg = [len(s.split()) for s in trg]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
